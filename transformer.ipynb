{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIoIlDcK6Hm65ChfrWgiF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sennatitcomb/contact-deeplearning/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers pandas torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E5p9I29QbvW",
        "outputId": "a3b8a571-d5f0-4298-c1fd-456c21220418"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YbBUzRWyQc7Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"bigshuffle.csv\")\n",
        "\n",
        "# Assuming your labels are in the \"Labels\" column\n",
        "labels = df[\"Labels\"].values\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "\n",
        "# Concatenate the columns into a single string for each row\n",
        "text_data = df[\"First Name\"] + \" \" + df[\"Last Name\"] + \" \" + df[\"Email\"] + \" \" + df[\"Phone\"] + \" \" + df[\"Title\"]\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_inputs = tokenizer(\n",
        "    text_data.tolist(),  # Convert to list of strings\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "# Create a PyTorch Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs, labels):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.tokenized_inputs[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.tokenized_inputs[\"attention_mask\"][idx],\n",
        "            \"labels\": torch.tensor(self.labels[idx]),\n",
        "        }\n",
        "\n",
        "dataset = CustomDataset(tokenized_inputs, labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Kdgp2lmzQh74"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "model_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=model_config)\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 8\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} Training\"):\n",
        "        inputs = {key: value.to(device) for key, value in batch.items() if key != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
        "        inputs = {key: value.to(device) for key, value in batch.items() if key != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy on the test set: {accuracy:.2%}\")\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"your_model_directory\")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"your_model_directory\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVR7k7-rQ9Ma",
        "outputId": "9d730cd0-783a-4a09-8e49-65bd0d7ce925"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1 Training: 100%|██████████| 62/62 [05:58<00:00,  5.78s/it]\n",
            "Epoch 2 Training: 100%|██████████| 62/62 [05:46<00:00,  5.58s/it]\n",
            "Epoch 3 Training: 100%|██████████| 62/62 [05:37<00:00,  5.44s/it]\n",
            "Testing: 100%|██████████| 16/16 [00:23<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 100.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('your_model_directory/tokenizer_config.json',\n",
              " 'your_model_directory/special_tokens_map.json',\n",
              " 'your_model_directory/vocab.txt',\n",
              " 'your_model_directory/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(\"your_model_directory\")\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained(\"your_model_directory\")\n",
        "\n",
        "# Example of using the model for predictions on new data\n",
        "new_data = pd.DataFrame({\n",
        "    \"First Name\": [\"John\"],\n",
        "    \"Last Name\": [\"Doe\"],\n",
        "    \"Email\": [\"john.doe@example.com\"],\n",
        "    \"Phone\": [\"123-456-7890\"],\n",
        "    \"Title\": [\"Data Scientist\"]\n",
        "})\n",
        "\n",
        "# Concatenate the columns into a single string for each row in the new data\n",
        "new_text_data = new_data[\"First Name\"] + \" \" + new_data[\"Last Name\"] + \" \" + new_data[\"Email\"] + \" \" + new_data[\"Phone\"] + \" \" + new_data[\"Title\"]\n",
        "\n",
        "# Tokenize the new data\n",
        "tokenized_new_data = loaded_tokenizer(\n",
        "    new_text_data.tolist(),  # Convert to list of strings\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    inputs = {key: value.to(device) for key, value in tokenized_new_data.items()}\n",
        "    outputs = loaded_model(**inputs)\n",
        "    _, predicted_label = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "\n",
        "print(f\"Predicted Label: {predicted_label.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHRhIBtzRPHE",
        "outputId": "f93ea43b-3d04-4697-a07a-4dd62459a921"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(\"your_model_directory\")\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained(\"your_model_directory\")\n",
        "\n",
        "# Example of using the model for predictions on new data\n",
        "new_data = pd.DataFrame({\n",
        "    \"First Name\": [\"@@&839@+7&\"],\n",
        "    \"Last Name\": [\"@@&839@+7&\"],\n",
        "    \"Email\": [\"@@&839@+7&\"],\n",
        "    \"Phone\": [\"@@&839@+7&\"],\n",
        "    \"Title\": [\"@@&839@+7&\"]\n",
        "})\n",
        "\n",
        "# Tokenize each column separately\n",
        "tokenized_first_name = loaded_tokenizer(new_data[\"First Name\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "tokenized_last_name = loaded_tokenizer(new_data[\"Last Name\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "tokenized_email = loaded_tokenizer(new_data[\"Email\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "tokenized_phone = loaded_tokenizer(new_data[\"Phone\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "tokenized_title = loaded_tokenizer(new_data[\"Title\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    inputs = {\n",
        "        \"input_ids\": tokenized_first_name[\"input_ids\"].to(device),\n",
        "        \"attention_mask\": tokenized_first_name[\"attention_mask\"].to(device),\n",
        "        # Add other tokenized tensors similarly\n",
        "    }\n",
        "    outputs = loaded_model(**inputs)\n",
        "    _, predicted_label = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "print(f\"Predicted Label: {predicted_label.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZfhaUJpSD_i",
        "outputId": "ba01cdb8-66b9-49b9-a1ba-fb04fd9db0d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the model and tokenizer\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(\"your_model_directory\")\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained(\"your_model_directory\")\n",
        "\n",
        "# Function to process and predict labels for a CSV file\n",
        "def process_csv_file(file_path):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Tokenize each column separately\n",
        "    tokenized_first_name = loaded_tokenizer(df[\"First Name\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    tokenized_last_name = loaded_tokenizer(df[\"Last Name\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    tokenized_email = loaded_tokenizer(df[\"Email\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    tokenized_phone = loaded_tokenizer(df[\"Phone\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    tokenized_title = loaded_tokenizer(df[\"Title\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        inputs = {\n",
        "            \"input_ids\": tokenized_first_name[\"input_ids\"].to(device),\n",
        "            \"attention_mask\": tokenized_first_name[\"attention_mask\"].to(device),\n",
        "            # Add other tokenized tensors similarly\n",
        "        }\n",
        "        outputs = loaded_model(**inputs)\n",
        "        _, predicted_labels = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "    # Display results\n",
        "    df[\"Predicted Label\"] = predicted_labels.cpu().numpy()\n",
        "    print(df)\n",
        "\n",
        "# Example usage\n",
        "csv_file_path = \"testdata.csv\"\n",
        "process_csv_file(csv_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA6lUP9QVLUL",
        "outputId": "9eb27406-d3ee-41bb-d915-4c7011dbea7d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    First Name   Last Name                          Email               Title  \\\n",
            "0         Axel        Howe        axel.howe@bilearner.com  Area Sales Manager   \n",
            "1       Milton        Wall      milton.wall@bilearner.com  Area Sales Manager   \n",
            "2         Cory    Robinson    cory.robinson@bilearner.com  Area Sales Manager   \n",
            "3       Saniya          Yu        saniya.yu@bilearner.com  Area Sales Manager   \n",
            "4        Alisa       James      alisa.james@bilearner.com  Area Sales Manager   \n",
            "5      Lincoln     Compton  lincoln.compton@bilearner.com  Area Sales Manager   \n",
            "6       Aliana       Nolan     aliana.nolan@bilearner.com  Area Sales Manager   \n",
            "7         Axel        Howe        axel.howe@bilearner.com  Area Sales Manager   \n",
            "8       Milton        Wall      milton.wall@bilearner.com  Area Sales Manager   \n",
            "9         Cory    Robinson    cory.robinson@bilearner.com  Area Sales Manager   \n",
            "10      Saniya          Yu        saniya.yu@bilearner.com  Area Sales Manager   \n",
            "11       Alisa       James      alisa.james@bilearner.com  Area Sales Manager   \n",
            "12     Lincoln     Compton  lincoln.compton@bilearner.com  Area Sales Manager   \n",
            "13      Aliana       Nolan     aliana.nolan@bilearner.com  Area Sales Manager   \n",
            "14        Axel        Howe        axel.howe@bilearner.com  Area Sales Manager   \n",
            "15  @6$0#6$09(  %=*)32)$73                    3*++26=38(@          -@=-!)+#6^   \n",
            "16  658%-&+(_6  $*02&6841*                    (2#^&%9(%7@          ##71&__!7=   \n",
            "17  27*+(*%42(  _++49!2_+(                    04@^0195$7@          ^=#)#9=^)(   \n",
            "18  +2(%%5+8@@  7$@5+&409@                    1=*($0+$77@          +2-68^$!@7   \n",
            "19  $6-%(_+-5*  @!^=9=1465                    )_%^%9^!7%@          (@$!7)%^#6   \n",
            "20  3-89)0$760  )4=563!^8^                    (=%=*5@7=1@          3%+$_3_656   \n",
            "21  =@559*0*_1  0%_-+39_3*                    80$34263^%@          )8^+=_(%(1   \n",
            "22  -%!@=5!0$7  #3+$3)22&-                    @%6650^@-5@          3+_=)--9&9   \n",
            "23  6@660=1)(=  $0!$08047@                    3+696925@0@          $2#2%=2^(!   \n",
            "\n",
            "           Phone  Predicted Label  \n",
            "0   395-809-2486                0  \n",
            "1   146-283-0646                0  \n",
            "2   255-030-6410                0  \n",
            "3   176-280-2355                0  \n",
            "4   485-815-7258                0  \n",
            "5   681-423-1382                0  \n",
            "6   232-428-5418                0  \n",
            "7   395-809-2486                0  \n",
            "8   146-283-0646                0  \n",
            "9   255-030-6410                0  \n",
            "10  176-280-2355                0  \n",
            "11  485-815-7258                0  \n",
            "12  681-423-1382                0  \n",
            "13  232-428-5418                0  \n",
            "14  395-809-2486                0  \n",
            "15   1_!)_@-*59$                1  \n",
            "16   1^&!3*(=!7$                1  \n",
            "17   --8%&$(^*5$                1  \n",
            "18   6$^7*&%$5_$                1  \n",
            "19   $5#-)=$$5$$                1  \n",
            "20   )#46*5@2%9$                1  \n",
            "21   42232#880!$                1  \n",
            "22   @14=7984+0$                1  \n",
            "23   43+-%4%!*#$                1  \n"
          ]
        }
      ]
    }
  ]
}